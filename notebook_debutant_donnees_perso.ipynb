{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igc5itf-xMGj"
   },
   "source": [
    "# Masakhane - Traduction Automatique pour les Langues Africaines (via JoeyNMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4fXCKCf36IK"
   },
   "source": [
    "## Note avant de debuter:\n",
    "### - L'idée est que vous soyez capable de faire des changements legers à ce code pour avoir un resultat pour votre corpus de traduction. \n",
    "\n",
    "### - tl;dr: Les commentaires **\"TODO\"** vous donnent des directives sur ce que vous devez modifier\n",
    "\n",
    "### - Si vous voulez avoir une idée de ce que vous faites, lisez le texte et jetez un coup d'œil aux liens.\n",
    "\n",
    "### - Avec 100 époques, l'exécution dans Google Colab devrait prendre environ 7 heures.\n",
    "\n",
    "### - Une fois que vous avez obtenu un résultat pour votre langue, veuillez joindre et envoyer par courriel votre notebook qui l'a généré à masakhanetranslation@gmail.com.\n",
    "\n",
    "### - Si vous y tenez et que vous en avez l'occasion, il serait formidable de faire un bref historique de votre langue. Voir les exemples dans [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
    "\n",
    "### - Ce notebook est destiné à être utilisé avec des données parallèles personnalisées. Cela signifie que vous avez besoin de deux fichiers, l'un dans votre langue, l'autre en anglais, et les lignes dans les fichiers sont des traductions correspondantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l929HimrxS0a"
   },
   "source": [
    "## Pré-traitez vos données\n",
    "\n",
    "Nous supposons ici que vous disposez déjà d'un jeu de données. Le format dans lequel nous allons le traiter ici exige que \n",
    "1. vous avez deux fichiers, un pour chaque langue\n",
    "2. les fichiers sont alignés sur les phrases, ce qui signifie que chaque ligne doit correspondre à la même ligne dans l'autre fichier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oGRmDELn7Az0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cn3tgQLzUxwn"
   },
   "outputs": [],
   "source": [
    "# TODO: Définissez vos langues source (source_language) et cible (target_language). Gardez à l'esprit que ces langues utilisent traditionnellement les codes de langue que l'on trouve ici :\n",
    "# Ceux-ci deviendront également les suffixes de tous les fichiers de vocabulaire et de corpus utilisés tout au long du projet.\n",
    "import os\n",
    "source_language = \"en\"\n",
    "target_language = \"xh\" \n",
    "lc = False  # Si Vrai, transformer les données en minuscules.\n",
    "seed = 42  # Graine aléatoire pour le mélange des données.\n",
    "tag = \"baseline\" # Donnez un nom unique à votre dossier - ceci afin de vous assurer que vous n'écrasez pas les modèles que vous avez déjà soumis.\n",
    "\n",
    "os.environ[\"src\"] = source_language # Les défini en bash également, puisque nous utilisons souvent des scripts bash\n",
    "os.environ[\"tgt\"] = target_language\n",
    "os.environ[\"tag\"] = tag\n",
    "\n",
    "# Ceci permet de l'enregistrer dans un dossier de notre gdrive !\n",
    "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
    "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBSgJHEw7Nvx"
   },
   "outputs": [],
   "source": [
    "!echo \"\"$gdrive_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gA75Fs9ys8Y9"
   },
   "outputs": [],
   "source": [
    "# Installer opus-tools\n",
    "! pip install opustools-pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xq-tDZVks7ZD"
   },
   "outputs": [],
   "source": [
    "# TODO: specifier les chemins vers les fichiers ici\n",
    "source_file = \"my_file.en\"\n",
    "target_file = \"my_file.xh\"\n",
    "\n",
    "# Ils doivent avoir la meme taille.\n",
    "! wc -l \"$source_file\"\n",
    "! wc -l \"$target_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pre-traitement! (OPTIONEL)\n",
    "\n",
    "# Si vos données contiennent des symboles bizarres ou autres, vous voudrez faire un peu de nettoyage et de normalisation.\n",
    "# Nous n'avons pas le code dans le notebook pour cela, mais vous pouvez utiliser sacremoses \"normalize\" par exemple pour normaliser la ponctuation : https://github.com/alvations/sacremoses.\n",
    "\n",
    "# Nous appliquons la tokénisation pour séparer les signes de ponctuation des mots réels, séparer les mots aux traits d'union, etc.\n",
    "# Si vos données sont déjà tokenisées, c'est génial ! Passez cette cellule.\n",
    "# Sinon, nous pouvons utiliser sacremoses pour faire la tokenisation pour nous. \n",
    "# Nous avons besoin que les données soient tokenisées de façon à ce qu'elles correspondent à l'ensemble de test global.\n",
    "\n",
    "! pip install sacremoses\n",
    "\n",
    "tok_source_file = source_file+\".tok\"\n",
    "tok_target_file = target_file+\".tok\"\n",
    "\n",
    "# Tokeniser la source\n",
    "! sacremoses -l \"$source_language\" tokenize < \"$source_file\" > \"$tok_source_file\"\n",
    "# Tokeniser la cible\n",
    "! sacremoses -l \"$target_language\" tokenize < \"$target_file\" > \"$tok_target_file\"\n",
    "\n",
    "# Regardons l'effet de la tokenisation sur le texte.\n",
    "! head \"$source_file\"*\n",
    "! head \"$target_file\"*\n",
    "\n",
    "# Modifiez les pointeurs vers nos fichiers de façon à ce que nous continuions à travailler avec les données tokenisées.\n",
    "source_file = tok_source_file\n",
    "target_file = tok_target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n48GDRnP8y2G"
   },
   "outputs": [],
   "source": [
    "# Telecharger le jeu de données de test global.\n",
    "! wget https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-any.en\n",
    "  \n",
    "# Et le jeu de données de test specifique pour cette paire de langues.\n",
    "os.environ[\"trg\"] = target_language \n",
    "os.environ[\"src\"] = source_language \n",
    "\n",
    "! wget https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
    "! mv test.en-$trg.en test.en\n",
    "! wget https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
    "! mv test.en-$trg.$trg test.$trg\n",
    "\n",
    "# TODO: Si cela échoue, cela signifie qu'il n'y a pas encore de jeu de test pour votre langue. C'est à vous d'en créer un.\n",
    "# Une bonne idée serait de prendre un sous-ensemble aléatoire de vos données, et de l'ajouter à https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-any.en.\n",
    "# Faites une Pull Request et faites-la approuvée et fusionnée.\n",
    "# Puis répétez cette cellule pour récupérer le nouvel ensemble de test.\n",
    "# Puis passez à la cellule suivante qui filtrera tous les doublons de l'ensemble d'entraînement, de sorte qu'il n'y ait pas de chevauchement entre l'ensemble d'entraînement et l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NqDG-CI28y2L"
   },
   "outputs": [],
   "source": [
    "# Lire les données de test pour filtrer les données d'entrainement et de développement.\n",
    "# Stocker la partie anglaise dans le jeu pour des vérifications rapides du filtrage.\n",
    "en_test_sents = set()\n",
    "filter_test_sents = \"test.en-any.en\"\n",
    "j = 0\n",
    "with open(filter_test_sents) as f:\n",
    "  for line in f:\n",
    "    en_test_sents.add(line.strip())\n",
    "    j += 1\n",
    "print(\"Chargement de {} phrases de test globales à filtrer à partir des données d'entrainement/développement REUSSI!\".format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3CNdwLBCfSIl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "source = []\n",
    "target = []\n",
    "skip_lines = []  # Collectez les numéros de ligne de la partie source pour sauter les mêmes lignes pour la partie cible.\n",
    "with open(source_file) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # Sauter les phrases qui sont contenues dans l'ensemble de test.\n",
    "        if line.strip() not in en_test_sents:\n",
    "            source.append(line.strip())\n",
    "        else:\n",
    "            skip_lines.append(i)             \n",
    "with open(target_file) as f:\n",
    "    for j, line in enumerate(f):\n",
    "        # Ajouter au corpus uniquement si la source correspondante n'a pas été sautée.\n",
    "        if j not in skip_lines:\n",
    "            target.append(line.strip())\n",
    "    \n",
    "print('Chargement de données réussi! \\nSaut de {}/{} lignes car contenues dans le jeu de test.'.format(len(skip_lines), i))\n",
    "    \n",
    "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkuK3B4p2AkN"
   },
   "source": [
    "## Pré-traitement et exportation\n",
    "\n",
    "C'est généralement une bonne idée de supprimer les traductions en double et les traductions contradictoires du corpus. En pratique, ces corpus publics en comportent un certain nombre qui doivent être nettoyés.\n",
    "\n",
    "De plus, nous allons diviser nos données dans dev/test/train et les exporter vers le système de fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_2ouEOH1_1q"
   },
   "outputs": [],
   "source": [
    "# supprimer les traductions en double\n",
    "df_pp = df.drop_duplicates()\n",
    "\n",
    "# abandonner les traductions contradictoires\n",
    "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
    "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
    "\n",
    "# Mélangez les données pour éliminer les biais dans la sélection du jeu de données de dev.\n",
    "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installez fuzzy wuzzy pour supprimer les phrases \"presque en double\" dans les jeux de test et d'entraînement.\n",
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein\n",
    "import time\n",
    "from fuzzywuzzy import process\n",
    "import numpy as np\n",
    "from os import cpu_count\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# réinitialiser l'index du jeu d'entrainement après le filtrage précédent\n",
    "df_pp.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# Supprimez les échantillons du jeu de données d'entrainement s'ils \"chevauchent presque\" \n",
    "# les échantillons de l'ensemble de test.\n",
    "\n",
    "# Fonction de filtrage. Ajustez le tampon (pad) pour réduire les correspondances \n",
    "# candidates dans un certain intervalle de longueur de caractères de l'échantillon donné.\n",
    "def fuzzfilter(sample, candidates, pad):\n",
    "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
    "  if len(candidates) > 0:\n",
    "    return process.extractOne(sample, candidates)[1]\n",
    "  else:\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "### itérer sur les lignes d'un dataframe pandas n'est pas recommandé, utilisons le traitement multiple pour appliquer la fonction.\n",
    "\n",
    "with Pool(cpu_count()-1) as pool:\n",
    "    scores = pool.map(partial(fuzzfilter, candidates=list(en_test_sents), pad=5), df_pp['source_sentence'])\n",
    "hours, rem = divmod(time.time() - start_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"done in {}h:{}min:{}seconds\".format(hours, minutes, seconds))\n",
    "\n",
    "# Filtrer les \"échantillons qui se chevauchent presque\".\n",
    "df_pp = df_pp.assign(scores=scores)\n",
    "df_pp = df_pp[df_pp['scores'] < 95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxxBOCA-xXhy"
   },
   "outputs": [],
   "source": [
    "# Cette section effectue la séparation entre train/dev pour les corpus parallèles et les enregistre dans des fichiers séparés.\n",
    "# Nous utilisons 1000 dev test et l'ensemble de test donné.\n",
    "import csv\n",
    "\n",
    "# TODO: si votre corpus est inférieur à 1000, réduisez ce nombre. Avec un corpus aussi petit, vous risquez de ne pas obtenir de bons résultats avec NMT :/\n",
    "# Faites le split entre dev/train et créez des corpus parallèles\n",
    "num_dev_patterns = 1000\n",
    "\n",
    "# Optionnel : mettre les corpus en minuscules - cela facilitera la généralisation, mais sans la casse correcte.\n",
    "if lc:  # Julia : rendre la minusculisation facultative\n",
    "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
    "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
    "\n",
    "# Julia: les jeux de test déjà générés\n",
    "dev = df_pp.tail(num_dev_patterns) # Herman: Erreur dans l'original\n",
    "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
    "\n",
    "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in stripped.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "    \n",
    "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
    "  for index, row in dev.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "\n",
    "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
    "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
    "\n",
    "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
    "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
    "\n",
    "\n",
    "# TODO: Vérifiez le format ci-dessous. Il ne doit pas y avoir de guillemets supplémentaires ou de caractères bizarres. Il ne doit pas non plus être vide.\n",
    "! head train.*\n",
    "! head dev.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epeCydmCyS8X"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Installation de JoeyNMT\n",
    "\n",
    "JoeyNMT est un paquet de Traduction Automatique Neuronale (NMT) simple et minimaliste, utile pour l'apprentissage et l'enseignement. Consultez la documentation de JoeyNMT [ici](https://joeynmt.readthedocs.io)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBRMm4kMxZ8L"
   },
   "outputs": [],
   "source": [
    "# Installer JoeyNMT\n",
    "! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install .\n",
    "# Installer Pytorch avec support GPU v1.7.1.\n",
    "! pip install torch==1.7.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaE77Tcppex9"
   },
   "source": [
    "# Pré-traitement des données en tokens BPE\n",
    "\n",
    "- L'une des améliorations les plus puissantes pour les langues agglutinantes (une caractéristique de la plupart des langues bantoues) est l'utilisation de la tokenisation BPE [(Sennrich, 2015)](https://arxiv.org/abs/1508.07909).\n",
    "\n",
    "- Il a également été démontré qu'en optimisant le nombre de codes BPE, nous améliorons considérablement les résultats pour les langues à faibles ressources [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
    "\n",
    "- Ci-dessous nous avons les scripts pour faire la tokenisation BPE de nos données. Nous utilisons 4000 tokens comme recommandé par [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). Vous n'avez pas besoin de modifier quoi que ce soit. Il suffit d'exécuter ce qui suit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-TyjtmXB1mL"
   },
   "outputs": [],
   "source": [
    "# L'utilisation d'une méthode différente de tokénisation a permis d'améliorer considérablement les performances de NMT. \n",
    "# Habituellement, NMT procède à la tokenisation par mots. Cependant, l'utilisation d'une méthode appelée BPE a permis d'améliorer considérablement les performances.\n",
    "\n",
    "# Faire un NMT de sous-mots\n",
    "from os import path\n",
    "os.environ[\"src\"] = source_language # Les définir en bash également, puisque nous utilisons souvent des scripts bash\n",
    "os.environ[\"tgt\"] = target_language\n",
    "\n",
    "# Apprendre les BPE sur les données d'entrainement.\n",
    "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
    "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
    "\n",
    "# Appliquer les splits BPE aux données de développement et de test.\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
    "\n",
    "# Créer un répertoire, déplacer tout ce à quoi nous tenons vers le bon endroit.\n",
    "! mkdir -p \"$data_path\"\n",
    "! cp train.* \"$data_path\"\n",
    "! cp test.* \"$data_path\"\n",
    "! cp dev.* \"$data_path\"\n",
    "! cp bpe.codes.4000 \"$data_path\"\n",
    "! ls \"$data_path\"\n",
    "\n",
    "# Déplacez également tout ce qui nous intéresse vers un emplacement monté dans google drive (pertinent si vous travaillez sur colab) à gdrive_path.\n",
    "! cp train.* \"$gdrive_path\"\n",
    "! cp test.* \"$gdrive_path\"\n",
    "! cp dev.* \"$gdrive_path\"\n",
    "! cp bpe.codes.4000 \"$gdrive_path\"\n",
    "! ls \"$gdrive_path\"\n",
    "\n",
    "# Créer le vocabulaire avec build_vocab\n",
    "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
    "\n",
    "# Echantillon de resultat\n",
    "! echo \"BPE Test language Sentences\"\n",
    "! tail -n 5 test.bpe.$tgt\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlMitUHR8Qy-"
   },
   "outputs": [],
   "source": [
    "# Déplacez également tout ce qui nous intéresse vers un emplacement monté dans google drive (pertinent si vous travaillez sur colab) à gdrive_path.\n",
    "! cp train.* \"$gdrive_path\"\n",
    "! cp test.* \"$gdrive_path\"\n",
    "! cp dev.* \"$gdrive_path\"\n",
    "! cp bpe.codes.4000 \"$gdrive_path\"\n",
    "! ls \"$gdrive_path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ixmzi60WsUZ8"
   },
   "source": [
    "# Creation de la config de JoeyNMT\n",
    "\n",
    "JoeyNMT nécessite une configuration yaml. Nous fournissons un modèle ci-dessous. Nous avons également défini un certain nombre de valeurs par défaut, avec lesquelles vous pouvez jouer !\n",
    "\n",
    "- Nous avons utilisé l'architecture Transformer \n",
    "- Nous avons fixé notre dropout à un niveau raisonnablement élevé : 0,3 (recommandé dans [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
    "\n",
    "Des choses avec lesquelles il vaut la peine de s'amuser :\n",
    "- La taille du lot (batch size) (il est également recommandé de la modifier pour les langues à faibles ressources).\n",
    "- Le nombre d'époques (epochs) (nous l'avons fixé à 30 pour qu'il fonctionne en une heure environ, à des fins de test).\n",
    "- Les options du décodeur (beam_size, alpha)\n",
    "- Les métriques d'évaluation (BLEU versus Crhf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIs1lY2hxMsl"
   },
   "outputs": [],
   "source": [
    "# Ceci crée le fichier de configuration pour notre système JoeyNMT. Comme cela peut sembler compliqué, nous avons fourni quelques paramètres utiles à mettre à jour.\n",
    "# (Vous pouvez bien sûr jouer avec tous les paramètres si vous le souhaitez !)\n",
    "\n",
    "name = '%s%s' % (source_language, target_language)\n",
    "gdrive_path = os.environ[\"gdrive_path\"]\n",
    "\n",
    "# Créer la config\n",
    "config = \"\"\"\n",
    "name: \"{name}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language}\"\n",
    "    train: \"data/{name}/train.bpe\"\n",
    "    dev:   \"data/{name}/dev.bpe\"\n",
    "    test:  \"data/{name}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"data/{name}/vocab.txt\"\n",
    "    trg_vocab: \"data/{name}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "    sacrebleu:                      # options sacrebleu\n",
    "        remove_whitespace: True     # Option `remove_whitespace` dans la fonction sacrebleu.corpus_chrf() (valeur par defaut: True)\n",
    "        tokenize: \"none\"            # Option `tokenize` dans la fonction sacrebleu.corpus_bleu() (les options incluent : \"none\" (à utiliser pour les données de test déjà tokenisées), \"13a\" (tokenizer minimal par défaut), \"intl\" qui fait surtout de la ponctuation et de l'unicode, etc). \n",
    "\n",
    "training:\n",
    "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: essayez de passer de plateau au scheduling de Noam\n",
    "    patience: 5                     # Pour le plateau : diminuer le taux d'apprentissage (learning rate) par le facteur de diminution (decrease_factor) si le score de validation ne s'est pas amélioré pendant ce nombre de tours de validation.\n",
    "    learning_rate_factor: 0.5       # facteur pour le scheduler Noam (utilisé avec Transformer)\n",
    "    learning_rate_warmup: 1000      # étapes d'échauffement pour le scheduler Noam (utilisé avec Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                     # TODO: Diminuer pour faire des essais et vérifier le fonctionnement. Une trentaine de minutes suffisent pour vérifier si le système fonctionne.\n",
    "    validation_freq: 1000          # TODO: Définir à au moins une fois par époque.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_transformer\"\n",
    "    overwrite: False               # TODO: Mettez la valeur True si vous voulez écraser les modèles éventuellement existants. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Augmentez à 8 pour les données plus volumineuses.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Augmentez à 512 pour les données plus volumineuses.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Augmentez à 512 pour les données plus volumineuses.\n",
    "        ff_size: 1024            # TODO: Augmentez à 2048 pour les données plus volumineuses.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Augmentez à 8 pour les données plus volumineuses.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Augmentez à 512 pour les données plus volumineuses.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Augmentez à 512 pour les données plus volumineuses.\n",
    "        ff_size: 1024            # TODO: Augmentez à 2048 pour les données plus volumineuses.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIifxE3Qzuvs"
   },
   "source": [
    "# Entrainer le Modele\n",
    "\n",
    "Cette simple ligne de joeynmt exécute l'entraînement en utilisant la configuration que nous avons faite ci-dessus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ZBPFwT94WpI"
   },
   "outputs": [],
   "source": [
    "# Entrainer le modèle\n",
    "# Vous pouvez appuyer sur Ctrl-C pour arrêter. Et ensuite, exécutez la cellule suivante pour sauvegarder vos points de contrôle !\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBoDS09JM807"
   },
   "outputs": [],
   "source": [
    "# Copiez les modèles créés depuis le stockage du notebook vers google drive pour un stockage permanent. \n",
    "! mkdir -p \"$gdrive_path/models/${src}${tgt}_transformer/\"\n",
    "! cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n94wlrCjVc17"
   },
   "outputs": [],
   "source": [
    "# Sortie de notre score de validation\n",
    "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66WhRE9lIhoD"
   },
   "outputs": [],
   "source": [
    "# Tester notre modèle\n",
    "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "starter_notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}